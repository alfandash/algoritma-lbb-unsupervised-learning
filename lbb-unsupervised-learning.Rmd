---
title: "Clustering Spotify Tracks"
author: "Alfan"
date: "3/8/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 10)
```

# Intro

## Background 

What is spotify? Spotify is a digital music, podcast, and video streaming service that gives you access to millions of songs and other content from artists all over the world. Recently spotify is one of biggest digital music and podcast service in world. 

Spotify definitely is one of tech company has very advanced technology. One of example is, each of song track have uploaded to platform, they will identified. We can get audio feature information for each track, and access very easy we can use this [Api link](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/). In this case we will using spotify dataset from API from this source [kaggle](https://www.kaggle.com/zaheenhamidani/ultimate-spotify-tracks-db).

We will try to analyze popularity for each track we get, based on data we will try to find there is relation from popularity with other feature or variable. We will also try to do clustering analysis using K-means method and for sure we will find try to reduction dimensionality using Principle Component Analysis (PCA)

## Dataset

We will use dataset we get from kaggle, can download from this source [source](https://www.kaggle.com/zaheenhamidani/ultimate-spotify-tracks-db))


## Intial Setup and Library

```{r message=FALSE}
# Starting collection for data science
library(tidyverse)
# Processing string
library(glue)
# Processing date data type
library(lubridate)
# Multivariate Data Analyses
library(factoextra)
# Multivariate Data Analyses
library(FactoMineR)
# Data visualization
library(ggplot2)
library(viridis)
library(GGally)
library(scales)
```


# Import Data

The dataset we download from kaggle, we will import dataset. This dataset contain audio feature from a track.

```{r message=FALSE}
tracks <- read_csv("data/SpotifyFeatures.csv")
```

Observe structure and preview imported dataset

```{r}
glimpse(tracks)
```

Variable Explanation:   
1. `genre` : Track genre   
2. `artist_name` : Artist name   
3. `track_name` : Title of track   
4. `track_id` : The Spotify ID for the track.   
5. `popularity` : Popularity rate (1-100)   
6. `acousticness` : A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.   
7. `danceability` : Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0.   
8. `duration_ms` : The duration of the track in milliseconds.   
9. `energy` : 	Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.   
10. `instrumentalness` : Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.   
11. `key` : The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.   
12. `liveness` : 	Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.   
13. `loudness` : The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.   
14. `mode` : 	Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.   
15. `speechiness` : Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.   
16, `tempo` : The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.   
17. `time_signature` : 	An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).   
18. `valence` : A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).   

# Data Wrangling

First of all, we would check NA or Empty value of each variable, We didn't find any NA inside data

```{r}
colSums(is.na(tracks))
```

Some variable have wrong type data, we need to convert the data type:

* `genre` : to factor   
* `key` : to factor   
* `genre` : to factor   
* `mode`: to factor

```{r}
tracks <- tracks  %>% 
                  mutate(genre = as.factor(genre),
                  key = as.factor(key),
                  genre = as.factor(str_replace_all(genre, "[[:punct:]]", "")),
                  mode = as.factor(mode))
```


Drop variable that we think didn't related with our case. In this case we prioritize variable with numerical data type.based on summary we will drop `track id`, `time_signature`, `track_name`. 

```{r}
summary(tracks)
```

```{r message=FALSE}
tracks <- tracks %>% select(-c(track_id,time_signature,track_name))
```

# Exploratory Data Analysis

From dataset we get, we found we have `genre` whish it we can group our data base on it. To make us focus on popularity variable, we would select 5 highest average genre. It can inteprate the genre have big distribution from low to highest popularity. We will visualize data: 

```{r}
genre_popularity <- tracks %>% select(popularity, genre) %>% group_by(genre) %>% summarise("average_popularity" = round(mean(popularity)))

ggplot(data=genre_popularity, mapping = aes(x = reorder(genre,average_popularity), y = average_popularity, fill = genre)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  theme(
    legend.position = "none",
    
  ) +
  labs(
    y = "Average popularity",
    x = "Genre"
  )
  
```

Top 5 Genre with highest average popularity is Pop, Rap, Rock, HipHop and Dance. We filter our dataset and select only this 5 genres.

```{r}
# Filter
tracks <- tracks %>% filter(genre == "Pop" | genre == "Rap" | genre == "Rock" | genre == "HipHop" | genre == "Dance")

# Total row
NROW(tracks)
```

We have filtered data, we can check distribution of popularity new data. We found distribution of popularity have spike in middle of popularity is around 50.

```{r}
hist(tracks$popularity)
```

## Clustering Opportunity

Before we use k-mean as method to clustering, we can use simple way to clustering some factor variable with popularity variable. Here we try to viasualize boxplot `popularity` with `key` and `genre`


```{r}
tracks %>% 
  ggplot(aes(x = key, y = popularity, fill = key)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  theme_minimal()
```

```{r}
tracks %>% 
  ggplot(aes(x = genre, y = popularity, fill = genre)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  theme_minimal()
```

From both bar, we can see in general `genre` and `key` didnt have significant relation to `popularity`. Even so, there is difference in plot `popluaritu` and `genre` when track overall using Key A# popularity slighlty higher and more have stable popularity than others.  

Other things, we found that Pop genre have more stable popluarity than other 4 genre. So we can consider an opinion, if producer want get more popularity in the spotify platform, we can make tracks song with Pop genre and overall key in tracks using A# key.

We only visualize `genre` and `key` before, and we found each correlation with popularity even though, cant significantly which variable can significantly increase popularity. So we will try visualize other numerical variable to see correlation between them.

```{r}
ggcorr(tracks, low = "blue", high = "red")
```

It show popularity dont have strong correlation with others any numberical variable. But we found some variable have strong each other, it indicates that this dataset has multicollinearity and might not suitable for various classification algorithms.

To find more interesting and undiscovered pattern in the data, we will use clustering method using the K-means. We will use Principal Component Analysis (PCA) can be performed for this data to produce non-multicollinearity data, while also reducing the dimension of the data and retaining as much as information possible. The result of this analysis can be utilized further for classification purpose with lower computation.

# Data Pre-processing

Since we will implement K-means method and using PCA, its need perform pre-processing data. We didnt take all data, we will do sampling from data. So i take around 5% from data.

```{r}
set.seed(100)
tracks_sample <- sample_n(tracks, (nrow(tracks) * 0.05))
NROW(tracks_sample)
```

After do sampling we check distribution of popularity. We get distribution our popularity frequency still have same pattern before we take sample.

```{r}
hist(tracks_sample$popularity)
```

Next step, we will do feature scaling.  Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. Normalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. So we need only numerical variable to do feature scaling.

```{r}
tracks_num <- tracks_sample %>% select(-c(genre,artist_name,key,mode))
str(tracks_num)
```

```{r}
tracks_scale <- scale(tracks_num)
head(tracks_scale)
```

# Clustering 

## Find optimal number of Clusters

We will try to find optimum number of cluster, in this case we will try use 3 method: Elbow Method, Sillhoute Method, and gap statistic. In the end after we get result from this 3 method, we will chose optimum clusters based on majority voting.

### Elbow Method

Best practice of elbow method is based on graph, we will choose are of "bend of an elbow"

```{r}
fviz_nbclust(tracks_num, kmeans, method = "wss", k.max = 10) +
  scale_y_continuous(labels = number_format(scale = 10^(-9), big.mark = ",", suffix = " bil.")) +
  labs(subtitle = "Elbow method")
```

We found 2 cluster is good enought since there is'nt significant decline in total within-cluster sum of squares on higher number of clusters. 

### Sillhouette Method

The silhouette method measures the silhouette coefficient, by calculating the mean intra-cluster distance and the mean nearest-cluster distance for each observations. We get the optimal number of clusters by choosing the number of cluster with the highest silhouette score (the peak).

```{r}
fviz_nbclust(tracks_num, kmeans, method = "silhouette", k.max = 10) 
```

Based on sillhouette method, number clusters with maximum score and considered as the optimum k-clusters is 2

### Gap Statistic

The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic.

```{r}
fviz_nbclust(tracks_num, kmeans, "gap_stat", k.max = 10) + labs(subtitle = "Gap Statistic method")
```


Based on gap statistic method, the optimal k is 1

### Majority Voting for Optimium K

Result of our 3 method is Elbow method k = 2, Sillhouette method k = 2, and Gap Statistic k = 1. Two of our method result is 2 we consider use this result, because if we choose k = 1, we cant analyze difference between culsters or segment.

## K-Means Clustering 

Here we implement optimal K from our process before, we decided using K = 2

```{r}
set.seed(100)
km_tracks <-kmeans(tracks_scale, centers = 2)
km_tracks
```

based on summary, we found 1558 observence goes to cluster 1 and other 736 goes to cluster 2. So right now we have information cluster of each observation, we can join cluster vector to our sample dataset

```{r}
tracks_sample$cluster <- as.factor(km_tracks$cluster)

head(tracks_sample)
```

## Cluster Analysis

We will do analysis and exploration base on cluster we already do using k-mean. our focus is popularity, let see is there correlation between cluster with our popularity rate.

```{r}
tracks_sample %>% 
  select(cluster, popularity) %>% 
  group_by(cluster) %>% 
  summarise_all("mean")
```

Between cluster 1 and 2 dont have any difference on Popularity in the average. it only show little difference that cluster 2 have 0.2 popularity more higher than cluster 1. Lets take look with boxplot

```{r}
tracks_sample %>% ggplot(aes(x = cluster, y = popularity, fill = cluster)) +
  geom_boxplot() +
  theme_minimal()
```

Same like we analyze before, there isnt sepecific difference popularity between both clusters. So we can assume that our cluster didnt focus on popularity feature or variable. lets take look by genre

```{r}

tracks_sample %>% 
  select(cluster, genre) %>% 
  group_by(genre, cluster) %>% 
  summarize(n = n()) %>% 
  ungroup() %>%
  spread(genre, n, fill=0)

```

Our cluster didnt seperate genre too. so lets take look other visualize `acousticness`

```{r}
tracks_sample %>% ggplot(aes(x = cluster, y = acousticness, fill = cluster)) +
  geom_boxplot() +
  theme_minimal()
```

`acousticness` is one of variable that cluster read it. so we can take look for overall variable to take look which other variable that our cluster specific different 

```{r}
tracks_sample %>%
  select_if(is.numeric) %>% 
  mutate(cluster = as.factor(km_tracks$cluster)) %>% 
  group_by(cluster) %>% 
  summarise_all("mean")
```

We can see between `Cluster 1`and `Cluster 2`  have some significant difference in feature `acousticness`, `energy`, `instrumentalness`, `liveness`, `loudness`, `energy`, `instrumentalness`, `loudness`. No matter Between cluster 1 and 2 dont have significant difference on popularity we can assume if Tracks have more patter like `Cluster 2 ` it have more chance to get better popularity



# PCA

Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. . PCA is sensitive to the relative scaling of the original variables.

## Dimensionality Reduction

We will try make PCA from our sample datasets. We can see eigenvalues and percentage of variance for each dimensions. Behaviour of Eivenvalues will more larger at first PCs and will goes down sequently to the end of PCs. 

```{r}
non_numeric <- which(sapply(tracks_sample, negate(is.numeric)))

tracks_pca <- PCA(tracks_sample,
                  scale.unit = T,
                  quali.sup = non_numeric,
                  graph = F,
                  ncp = 11)

summary(tracks_pca)
```


After we finish make PCA to make more easy understand we can visualize each variance captured by each dimensions.

```{r}
fviz_eig(tracks_pca, ncp = 11, addlabels = T, main = "Variance Explained by Dimensions")
```

We can see around 50% variances of sample data can be explained only by first 4 dimensions.

Our target is to reduction feature to make more light computation, so we can target more than 80% information with minimum dimension. We can take 8 dimensions from 11 dimensions we have, because we can sum up, with only 8 dimensions we can reach more than 80% total variance can representate our dataset.

```{r}
tracks_pca_min <- data.frame(tracks_pca$ind$coord[ ,1:8]) %>% 
  bind_cols(cluster = as.factor(tracks_sample$cluster))

head(tracks_pca_min)
```

```{r}
pca_dimdesc<-dimdesc(tracks_pca)
```

```{r}
pca_dimdesc$Dim.1$quanti
```

Energy and loudness 2 variables that give more contribute on PC 1. so from PC 1 we can get more information of this both variables

## Individual and Variable Factor Map

From the previous section, we have discussed that PCA can be combined with clustering to obtain better visualization of our clustering result, or simply to understand the pattern in our dataset. 

```{r}
fviz_cluster(object = km_tracks, data = tracks_scale) + 
  theme_minimal()
```

The plots above are examples of individual factor map. The points in the plot resemble observations and colored by Cluster (Kernel by clustering result). Dim1 and Dim2 are PC1 and PC2 respectively, with their own share (percentage) of information from the total information of the dataset. With only this visualize we cant really understand what kind insight of this pattern. We can add other visualize Variable Factor Map

```{r}
fviz_pca_var(tracks_pca) +
  theme_minimal()
```

The plot above shows us that the variables are located inside the circle, meaning that we would need more that two components to represent our data perfectly. The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are correlated with PC1 and PC2 are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis.

Some insight we can get from Individual Factor Map and Variable Factor Map

Cluster 1: tracks that have more `danceability`,`speechiness`, `valence`,`loudness`,`energy`,`tempo` and `lieveness` and it little more better `popularity` than Cluster 1
Cluster 2: tracks have more `accousticness` `instrumentalness` and more `durations`


## PCA Clustering

PCA can also be integrated with the result of the K-means Clustering to help visualize our data in a fewer dimensions than the original features.

```{r}
fviz_pca_ind(tracks_pca, habillage = 1)
```


Some insight we can get from this visualize is :   
1. all genre have 2 different cluster   
2. Looks from Variable Factor Map, Genre rock have chance get more smaller popularity than other 4 genre
3. Pop and Hiphop genre with more `danceability`,`speechiness`, `valence` have more bigger chance get more better Popularity

# Conclusion

From the unsupervised learning analysis above, we can summarize:

1. `Popularity` only have small related with other feature/variable. but have more `danceability`,`speechiness`, `valence` can help get more better `Popularity`
2. Based on clustering, tracks that to be `Cluster 1` get more better chance to get better `Popularity` than `Cluster 2`
3. Rock genre that typically more `loudness`,`energy`,`tempo`, `lieveness`, `instrumentalness` and `durations` looks get smaller popularity than others genre
4. Dimensionality reduction can be performed using this dataset. To perform dimensionality reduction, we can pick PCs from a total of 11 PC according to the total information. we can target more than 80% information with minimum dimension. We can take 8 dimensions from 11 dimensions we have, because we can sum up, with only 8 dimensions we can reach more than 80% total variance can representative our dataset.
5. The improved data set obtained from unsupervised learning (eg.PCA) can be utilized further for supervised learning (classification) or for better data visualization (high dimensional data) with various insights.


